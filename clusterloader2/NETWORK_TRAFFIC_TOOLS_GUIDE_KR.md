# 네트워크 트래픽 생성 도구와 측정 원리 완벽 가이드

> ClusterLoader2에서 사용하는 네트워크 테스트 도구들의 동작 원리와 선택 기준

---

## 목차

1. [트래픽 생성의 기본 원리](#1-트래픽-생성의-기본-원리)
2. [측정 도구 상세: iperf란?](#2-측정-도구-상세-iperf란)
3. [측정 도구 상세: Siege란?](#3-측정-도구-상세-siege란)
4. [Worker Pod의 동작 원리](#4-worker-pod의-동작-원리)
5. [통신 테스트는 어떻게 이루어지나?](#5-통신-테스트는-어떻게-이루어지나)
6. [결과 판정: 언제 통과이고 언제 실패인가?](#6-결과-판정-언제-통과이고-언제-실패인가)
7. [Fortio vs 현재 도구들: 무엇을 선택해야 하나?](#7-fortio-vs-현재-도구들-무엇을-선택해야-하나)
8. [인수 테스트로의 적합성](#8-인수-테스트로의-적합성)

---

## 1. 트래픽 생성의 기본 원리

### 1.1 네트워크 성능 테스트란?

네트워크 성능 테스트는 **"A에서 B로 데이터를 보낼 때 얼마나 잘 전달되는가?"**를 측정합니다.

```
┌─────────────────────────────────────────────────────────────────┐
│                    네트워크 성능의 4가지 측면                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 대역폭 (Bandwidth/Throughput)                               │
│     └── "한 번에 얼마나 많이 보낼 수 있나?"                      │
│     └── 단위: Mbps, kbytes/sec                                  │
│     └── 비유: 고속도로의 차선 수                                 │
│                                                                 │
│  2. 지연시간 (Latency)                                          │
│     └── "보내서 도착하기까지 얼마나 걸리나?"                     │
│     └── 단위: ms (밀리초)                                       │
│     └── 비유: 서울에서 부산까지 걸리는 시간                      │
│                                                                 │
│  3. 지터 (Jitter)                                               │
│     └── "도착 시간이 얼마나 일정한가?"                          │
│     └── 단위: ms                                                │
│     └── 비유: 버스가 정시에 오는가, 들쭉날쭉한가                 │
│                                                                 │
│  4. 패킷 손실 (Packet Loss)                                     │
│     └── "보낸 것 중 얼마나 잃어버리나?"                         │
│     └── 단위: %                                                 │
│     └── 비유: 택배 분실률                                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 1.2 프로토콜별 특성

| 프로토콜 | 특징 | 측정 가능 항목 | 실제 사용 예 |
|----------|------|----------------|--------------|
| **TCP** | 신뢰성 보장, 순서 보장 | 대역폭 | 파일 다운로드, API 호출 |
| **UDP** | 신뢰성 없음, 빠름 | 지연, 지터, 손실 | 게임, VoIP, 스트리밍 |
| **HTTP** | 애플리케이션 레벨 | 응답시간, 처리량 | 웹 서비스, REST API |

### 1.3 클라이언트-서버 모델

모든 네트워크 테스트는 **클라이언트(보내는 쪽)**와 **서버(받는 쪽)**가 필요합니다:

```
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│     클라이언트 Pod                    서버 Pod                 │
│     ┌───────────────┐                ┌───────────────┐        │
│     │               │                │               │        │
│     │  "데이터를     │ ──────────►   │  "데이터를     │        │
│     │   보내는 역할" │   네트워크    │   받는 역할"   │        │
│     │               │               │               │        │
│     │  트래픽 생성   │               │  트래픽 수신   │        │
│     │               │               │  + 측정       │        │
│     └───────────────┘               └───────────────┘        │
│                                                                │
│     실행하는 명령:                   실행하는 명령:            │
│     iperf -c <서버IP>               iperf -s                  │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

**중요**: 측정 결과는 **서버 Pod에서 수집**됩니다 (TCP/UDP의 경우).
HTTP의 경우는 반대로 **클라이언트 Pod에서 수집**됩니다.

---

## 2. 측정 도구 상세: iperf란?

### 2.1 iperf 소개

**iperf**는 네트워크 대역폭 측정을 위한 **업계 표준 도구**입니다.

```
┌─────────────────────────────────────────────────────────────────┐
│                         iperf 개요                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  개발: NLANR/DAST (University of California, San Diego)         │
│  역사: 1999년부터 개발, 20년 이상 사용                          │
│  버전:                                                          │
│    - iperf2 (2.0.x): UDP 측정에 더 많은 메트릭 제공             │
│    - iperf3 (3.x): 새로운 아키텍처, 단일 스레드                 │
│                                                                 │
│  ClusterLoader2는 iperf2 (2.0.9)를 사용                         │
│  이유: UDP 측정 시 jitter, latency 등 더 상세한 메트릭 제공     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 iperf가 측정하는 방법

#### TCP 측정 원리

```
┌─────────────────────────────────────────────────────────────────┐
│                     TCP 대역폭 측정 원리                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 클라이언트가 서버에 TCP 연결 수립                            │
│                                                                 │
│  2. 클라이언트가 최대 속도로 데이터 전송                         │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ ████████████████████████████████████████ → 데이터   │    │
│     │ ████████████████████████████████████████ → 전송     │    │
│     │ ████████████████████████████████████████ → 계속     │    │
│     └─────────────────────────────────────────────────────┘    │
│                                                                 │
│  3. 서버가 받은 데이터량 측정                                    │
│                                                                 │
│  4. 전송량 / 시간 = 대역폭 (kbytes/sec)                         │
│                                                                 │
│  예: 10초 동안 50MB 전송 → 5MB/s = 5000 kbytes/sec              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**실제 실행 명령:**

```bash
# 서버 (받는 쪽)
iperf -s -f K -i 10 -P 1
# -s: 서버 모드
# -f K: 결과를 KBytes/sec로 표시
# -i 10: 10초마다 리포트
# -P 1: 1개 클라이언트 처리 후 종료

# 클라이언트 (보내는 쪽)
iperf -c 10.0.0.100 -f K -i 1 -t 10
# -c: 클라이언트 모드 (서버 IP 지정)
# -t 10: 10초 동안 테스트
```

#### UDP 측정 원리

```
┌─────────────────────────────────────────────────────────────────┐
│                     UDP 성능 측정 원리                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  UDP는 TCP와 달리 "보내고 잊어버리는" 방식입니다.                 │
│  따라서 더 많은 측정 항목이 있습니다.                            │
│                                                                 │
│  클라이언트 ────────────────────────────────────► 서버           │
│     │                                              │            │
│     │  패킷 #1 ─────────────────────────────►     │            │
│     │  패킷 #2 ─────────────────────────────►     │ 도착       │
│     │  패킷 #3 ──────────────X (손실)             │            │
│     │  패킷 #4 ─────────────────────────────►     │ 도착       │
│     │  패킷 #5 ─────────────────────────────►     │ 도착       │
│     │                                              │            │
│                                                                 │
│  서버가 측정하는 것:                                             │
│  1. 몇 개 받았나? (패킷 수)                                      │
│  2. 몇 개 잃어버렸나? (손실률)                                   │
│  3. 도착 간격이 일정했나? (Jitter)                               │
│  4. 얼마나 걸렸나? (Latency)                                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Jitter (지터)란?**

```
이상적인 경우 (Jitter = 0):
패킷 도착: ──●────●────●────●────●──
              10ms  10ms  10ms  10ms  (일정한 간격)

Jitter가 높은 경우:
패킷 도착: ──●──●──────●─●──────●──
              5ms  15ms  3ms  20ms  (불규칙한 간격)

계산: 평균 도착 간격에서 각 패킷의 편차의 평균
      Jitter = |d1-d_avg| + |d2-d_avg| + ... / n
```

**실제 실행 명령:**

```bash
# 서버
iperf -s -f K -u -e -i 10 -P 1
# -u: UDP 모드
# -e: Enhanced(확장) 리포트 - jitter, latency 등 추가 측정

# 클라이언트
iperf -c 10.0.0.100 -u -f K -e -i 1 -t 10 -b 750M
# -b 750M: 750Mbps 속도로 전송 (UDP는 속도 제한 필요)
```

### 2.3 iperf 출력 예시와 해석

**TCP 출력:**
```
------------------------------------------------------------
Server listening on TCP port 5001
TCP window size: 85.3 KByte (default)
------------------------------------------------------------
[  4] local 10.0.0.100 port 5001 connected with 10.0.0.50 port 54321
[ ID] Interval       Transfer     Bandwidth
[  4]  0.0-10.0 sec  48.5 MBytes  4867 KBytes/sec
                                  ↑
                                  이 값이 Throughput
```

**UDP 출력 (-e 옵션):**
```
[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams
[  3]  0.0-10.0 sec  89.4 MBytes  8940 KBytes/sec  0.043 ms 15/63877 (0.023%)
                                                    ↑         ↑
                                                    Jitter    패킷 손실률

read failed: Connection refused
Server Report:
[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total  Latency avg/min/max/stdev PPS
[  3]  0.0-10.0 sec  89.4 MBytes  8940 KBytes/sec  0.043 ms 15/63877    0.156/0.029/5.234/0.087 ms  6387 pps
                                                                        ↑                          ↑
                                                                        Latency 통계               초당 패킷 수
```

### 2.4 ClusterLoader2에서의 iperf 메트릭 인덱스

Worker Pod는 iperf 출력을 파싱해서 배열로 저장합니다:

```go
// TCP: 2개 메트릭
// Index 0: (사용 안 함)
// Index 1: Bandwidth (kbytes/sec) ← 이것만 사용

// UDP: 11개 메트릭
// Index 0: (사용 안 함)
// Index 1: (사용 안 함)
// Index 2: Jitter (ms)           ← 사용
// Index 3: (전송량)
// Index 4: (전송량)
// Index 5: Lost Packets (%)      ← 사용
// Index 6: Latency Average (ms)  ← 사용
// Index 7-9: (Latency min/max/stdev)
// Index 10: Packets Per Second   ← 사용
```

---

## 3. 측정 도구 상세: Siege란?

### 3.1 Siege 소개

**Siege**는 HTTP 부하 테스트 도구입니다. 웹 서버에 동시 요청을 보내고 성능을 측정합니다.

```
┌─────────────────────────────────────────────────────────────────┐
│                         Siege 개요                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  개발: Joe Dog Software                                         │
│  버전: 3.1.4 (ClusterLoader2에서 사용)                          │
│                                                                 │
│  목적:                                                          │
│  - 웹 서버 스트레스 테스트                                       │
│  - HTTP 응답 시간 측정                                          │
│  - 동시 사용자 시뮬레이션                                       │
│                                                                 │
│  iperf와의 차이:                                                │
│  - iperf: OSI 4계층 (Transport Layer) - TCP/UDP 원시 성능       │
│  - Siege: OSI 7계층 (Application Layer) - HTTP 애플리케이션 성능│
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 Siege가 측정하는 방법

```
┌─────────────────────────────────────────────────────────────────┐
│                     HTTP 성능 측정 원리                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Siege (Client)                    HTTP Server (Server Pod)    │
│  ┌─────────────┐                   ┌─────────────────────┐     │
│  │             │                   │                     │     │
│  │  GET /test  │───── 요청 ───────►│  HTTP Server        │     │
│  │             │                   │  (Port 5301)        │     │
│  │  t_start    │                   │                     │     │
│  │             │◄──── 응답 ────────│  {"ok"}             │     │
│  │  t_end      │                   │                     │     │
│  │             │                   │                     │     │
│  │ Response    │                   └─────────────────────┘     │
│  │ Time =      │                                               │
│  │ t_end -     │                                               │
│  │ t_start     │                                               │
│  └─────────────┘                                               │
│                                                                 │
│  이 과정을 수천 번 반복하여 통계 생성                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**실제 실행 명령:**

```bash
# Siege 클라이언트
siege http://10.0.0.100:5301/test -d1 -t10S -c1
# -d1: 요청 사이 랜덤 딜레이 (0~1초)
# -t10S: 10초 동안 테스트
# -c1: 동시 사용자 1명
```

### 3.3 Siege 출력 예시와 해석

```
** SIEGE 3.1.4
** Preparing 1 concurrent user for battle.
The server is now under siege...
Lifting the server siege...

Transactions:                    892 hits        ← 완료된 요청 수
Availability:                  100.00 %          ← 성공률
Elapsed time:                    9.89 secs       ← 전체 테스트 시간
Data transferred:                0.00 MB         ← 전송된 데이터량
Response time:                   0.01 secs       ← 평균 응답 시간 ★
Transaction rate:               90.19 trans/sec  ← 초당 처리량
Throughput:                      0.00 MB/sec     ← 데이터 처리량
Concurrency:                     0.99            ← 실제 동시성
Successful transactions:          892            ← 성공한 요청
Failed transactions:               0             ← 실패한 요청
Longest transaction:             0.04            ← 가장 긴 응답
Shortest transaction:            0.00            ← 가장 짧은 응답
```

**ClusterLoader2가 사용하는 핵심 메트릭:** `Response time` (Index 4)

---

## 4. Worker Pod의 동작 원리

### 4.1 Worker Pod란?

**Worker Pod**는 `gcr.io/k8s-testimages/netperfbenchmark:0.3` 이미지로 실행되는 Pod입니다.

```
┌─────────────────────────────────────────────────────────────────┐
│                     Worker Pod 내부 구조                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              netperfbenchmark Container                  │   │
│  │                                                          │   │
│  │  ┌──────────────────────────────────────────────────┐   │   │
│  │  │ Go 바이너리 (/workspace/network)                  │   │   │
│  │  │                                                   │   │   │
│  │  │  • Kubernetes API 연결                            │   │   │
│  │  │  • NetworkTestRequest CR 감시                     │   │   │
│  │  │  • 테스트 명령 실행                               │   │   │
│  │  │  • 결과 파싱 및 보고                              │   │   │
│  │  └──────────────────────────────────────────────────┘   │   │
│  │                                                          │   │
│  │  ┌────────────────┐  ┌────────────────┐                 │   │
│  │  │ iperf 2.0.9    │  │ siege 3.1.4    │                 │   │
│  │  │ (TCP/UDP 측정)  │  │ (HTTP 측정)    │                 │   │
│  │  └────────────────┘  └────────────────┘                 │   │
│  │                                                          │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 Worker Pod 시작 시 무슨 일이 일어나나?

```go
// cmd/main.go (단순화)
func main() {
    worker.NewWorker().Start(extraArgumentsMap)
}

// 실행 흐름:
// 1. POD_NAME 환경변수에서 자신의 Pod 이름 확인
// 2. Kubernetes Dynamic Client 생성
// 3. NetworkTestRequest CR을 감시하는 Informer 시작
// 4. 자신의 Pod 이름이 포함된 CR만 필터링
// 5. CR이 생성되면 handleCustomResource() 호출
```

### 4.3 CR 이벤트 처리 흐름

```
┌─────────────────────────────────────────────────────────────────┐
│              CR 이벤트 처리 상세 흐름                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. CR 수신                                                     │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ NetworkTestRequest:                                  │    │
│     │   clientPodName: worker-abc                         │    │
│     │   serverPodName: worker-xyz                         │    │
│     │   serverPodIP: 10.0.0.100                          │    │
│     │   protocol: TCP                                     │    │
│     │   duration: 10                                      │    │
│     │   clientStartTimestamp: 1699999999                 │    │
│     └─────────────────────────────────────────────────────┘    │
│                              │                                  │
│                              ▼                                  │
│  2. 역할 판단                                                   │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ if POD_NAME == clientPodName:                       │    │
│     │     workType = "client"                             │    │
│     │ elif POD_NAME == serverPodName:                     │    │
│     │     workType = "server"                             │    │
│     └─────────────────────────────────────────────────────┘    │
│                              │                                  │
│                              ▼                                  │
│  3. 명령 템플릿 채우기                                          │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ 템플릿: ["-c", "{serverPodIP}", "-t", "{duration}"] │    │
│     │                    ↓                                │    │
│     │ 결과:   ["-c", "10.0.0.100", "-t", "10"]            │    │
│     └─────────────────────────────────────────────────────┘    │
│                              │                                  │
│                              ▼                                  │
│  4. 스케줄링 (동기화)                                           │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ startTime = time.Unix(clientStartTimestamp, 0)      │    │
│     │ time.Sleep(startTime.Sub(time.Now()))               │    │
│     │                                                      │    │
│     │ // 모든 Pod가 동시에 시작하도록 대기                  │    │
│     └─────────────────────────────────────────────────────┘    │
│                              │                                  │
│                              ▼                                  │
│  5. 명령 실행                                                   │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ exec.Command("iperf", arguments...)                 │    │
│     │                                                      │    │
│     │ 실행: iperf -c 10.0.0.100 -f K -i 1 -t 10           │    │
│     └─────────────────────────────────────────────────────┘    │
│                              │                                  │
│                              ▼                                  │
│  6. 결과 파싱                                                   │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ parseIperfResponse(output) → [5000.0, ...]          │    │
│     └─────────────────────────────────────────────────────┘    │
│                              │                                  │
│                              ▼                                  │
│  7. CR Status 업데이트                                          │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ status:                                              │    │
│     │   metrics: [5000.0]                                  │    │
│     │   workerDelay: 0.05                                  │    │
│     └─────────────────────────────────────────────────────┘    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.4 누가 결과를 보고하나?

**프로토콜별로 다릅니다:**

| 프로토콜 | 결과 보고 주체 | 이유 |
|----------|----------------|------|
| TCP | 서버 Pod | 서버가 받은 데이터량 측정 |
| UDP | 서버 Pod | 서버가 패킷 손실, 지터 측정 |
| HTTP | 클라이언트 Pod | 클라이언트가 응답 시간 측정 |

```go
// worker.go의 shouldParseResponse() 함수
func (w *Worker) shouldParseResponse() bool {
    return (w.work.resourceSpec.Protocol != ProtocolHTTP && w.work.workType == "server") ||
           (w.work.resourceSpec.Protocol == ProtocolHTTP && w.work.workType == "client")
}
```

### 4.5 HTTP 서버 모드

HTTP 프로토콜에서 서버 Pod는 iperf 대신 **내장 HTTP 서버**를 실행합니다:

```go
// worker.go
func (w *Worker) StartHTTPServer() {
    go w.startListening("5301", handlersMap{"/test": w.Handler})
}

func (w *Worker) Handler(rw http.ResponseWriter, _ *http.Request) {
    w.sendResponse(rw, http.StatusOK, "ok")  // 단순히 "ok" 응답
}
```

---

## 5. 통신 테스트는 어떻게 이루어지나?

### 5.1 전체 테스트 시퀀스

```
┌─────────────────────────────────────────────────────────────────┐
│                    테스트 전체 시퀀스 다이어그램                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ClusterLoader2          Worker Pod A          Worker Pod B     │
│       │                       │                      │          │
│       │  1. Worker 배포       │                      │          │
│       │ ─────────────────────►│                      │          │
│       │ ─────────────────────────────────────────────►          │
│       │                       │                      │          │
│       │  2. Pod Ready 대기    │                      │          │
│       │ ◄─────────────────────│                      │          │
│       │ ◄────────────────────────────────────────────│          │
│       │                       │                      │          │
│       │  3. 시작 시간 계산    │                      │          │
│       │  (now + 15초)        │                      │          │
│       │                       │                      │          │
│       │  4. CR 생성          │                      │          │
│       │   (clientPod: A,     │                      │          │
│       │    serverPod: B,     │                      │          │
│       │    startTime: T)     │                      │          │
│       │ ─────────────────────►│                      │          │
│       │ ─────────────────────────────────────────────►          │
│       │                       │                      │          │
│       │                       │  5. CR 수신, 역할 판단           │
│       │                       │  "나는 Client"        │          │
│       │                       │                      │  "나는 Server"
│       │                       │                      │          │
│       │                       │  6. 시간 T까지 대기  │          │
│       │                       │                      │          │
│       │                       │  ═══════ 시간 T ═════════════   │
│       │                       │                      │          │
│       │                       │  7. 동시에 실행!     │          │
│       │                       │  iperf -c ...        │          │
│       │                       │ ────────────────────►│ iperf -s │
│       │                       │      트래픽          │          │
│       │                       │ ────────────────────►│          │
│       │                       │ ────────────────────►│          │
│       │                       │                      │          │
│       │                       │  8. 10초 후 완료     │          │
│       │                       │                      │          │
│       │                       │                      │  9. 결과 파싱
│       │                       │                      │  metrics: [5000]
│       │                       │                      │          │
│       │  10. CR status 업데이트                      │          │
│       │ ◄────────────────────────────────────────────│          │
│       │                       │                      │          │
│       │  11. 결과 수집 완료   │                      │          │
│       │                       │                      │          │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 동기화의 중요성

**왜 startTimestamp가 필요한가?**

```
문제 상황 (동기화 없이):
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  Pod A (Client): CR 받음 → 즉시 iperf 실행 → 연결 실패!        │
│  Pod B (Server): CR 받음 → 2초 후 iperf 실행 → 이미 늦음      │
│                                                                │
│  시간: ──●─────────────●───────────────────────────────        │
│         │             │                                        │
│         A 시작        B 시작 (너무 늦음)                        │
│                                                                │
└────────────────────────────────────────────────────────────────┘

해결책 (동기화):
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  ClusterLoader2: "15초 후인 T에 시작해라"                       │
│                                                                │
│  Pod A: CR 받음 → T까지 대기 → 동시 시작!                      │
│  Pod B: CR 받음 → T까지 대기 → 동시 시작!                      │
│                                                                │
│  시간: ────────────────●────────────────────────               │
│                        │                                       │
│                        A, B 동시 시작                          │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

### 5.3 Pod 페어링 전략

**왜 다른 노드의 Pod끼리 페어링하나?**

```
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  같은 노드 내 통신:                                            │
│  ┌─────────────────────────────────────────────────────────┐  │
│  │  Node A                                                  │  │
│  │  ┌─────────┐      ┌─────────┐                           │  │
│  │  │ Pod 1   │─────►│ Pod 2   │                           │  │
│  │  └─────────┘      └─────────┘                           │  │
│  │                                                          │  │
│  │  특징:                                                   │  │
│  │  • 네트워크를 거의 안 탐                                  │  │
│  │  • 메모리 복사 수준의 빠른 통신                           │  │
│  │  • CNI 오버헤드 최소화                                    │  │
│  │  • 실제 서비스 환경과 다름!                               │  │
│  └─────────────────────────────────────────────────────────┘  │
│                                                                │
│  다른 노드 간 통신 (실제 테스트):                              │
│  ┌─────────────────────────────────────────────────────────┐  │
│  │  Node A                    Node B                        │  │
│  │  ┌─────────┐              ┌─────────┐                   │  │
│  │  │ Pod 1   │──────────────│ Pod 2   │                   │  │
│  │  └─────────┘   실제 NW   └─────────┘                   │  │
│  │                                                          │  │
│  │  특징:                                                   │  │
│  │  • 실제 물리 네트워크 경유                                │  │
│  │  • CNI 오버레이 (VXLAN, Geneve 등) 오버헤드 포함         │  │
│  │  • 실제 서비스 환경과 동일                                │  │
│  └─────────────────────────────────────────────────────────┘  │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

---

## 6. 결과 판정: 언제 통과이고 언제 실패인가?

### 6.1 ClusterLoader2의 기본 판정

**중요: ClusterLoader2 자체는 Pass/Fail을 자동으로 판정하지 않습니다!**

```
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  ClusterLoader2의 역할:                                        │
│                                                                │
│  1. 테스트 실행                                                │
│  2. 메트릭 수집                                                │
│  3. JSON 형식으로 결과 저장                                    │
│                                                                │
│  Pass/Fail 판정은 사용자가 결과를 보고 결정해야 함             │
│  또는 별도의 threshold 설정 필요                               │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

### 6.2 실패로 간주되는 경우

**테스트 실행 자체가 실패하는 경우:**

| 실패 유형 | 원인 | CR status |
|-----------|------|-----------|
| Pod 생성 실패 | 이미지 pull 실패, 리소스 부족 | 테스트 시작 불가 |
| 연결 실패 | 서버가 리스닝하지 않음, 네트워크 차단 | `error: "connection refused"` |
| 타임아웃 | 응답이 오지 않음 | `error: "timeout"` |
| 파싱 실패 | iperf 출력 형식 오류 | `error: "parsing failed"` |

### 6.3 SLA/Threshold 기반 판정 예시

**인수 테스트에서 사용할 수 있는 기준:**

```yaml
# 예시: 커스텀 threshold 정의 (사용자가 별도 구현 필요)

network_performance_sla:
  tcp:
    throughput_min_kbytes: 1000   # 최소 1000 KB/s

  udp:
    jitter_max_ms: 5              # Jitter 5ms 이하
    packet_loss_max_percent: 0.1  # 손실률 0.1% 이하
    latency_max_ms: 10            # 지연시간 10ms 이하

  http:
    response_time_max_sec: 1      # 응답시간 1초 이하
```

**판정 로직 예시 (Python):**

```python
def check_network_performance(results, sla):
    """결과가 SLA를 만족하는지 검사"""

    for item in results['dataItems']:
        metric = item['labels']['Metric']

        if metric == 'Throughput':
            perc50 = item['data'].get('Perc50') or item['data'].get('Value')
            if perc50 < sla['tcp']['throughput_min_kbytes']:
                return False, f"Throughput {perc50} < {sla['tcp']['throughput_min_kbytes']}"

        elif metric == 'Jitter':
            perc95 = item['data'].get('Perc95') or item['data'].get('Value')
            if perc95 > sla['udp']['jitter_max_ms']:
                return False, f"Jitter {perc95} > {sla['udp']['jitter_max_ms']}"

    return True, "All checks passed"
```

### 6.4 NetworkPolicy 테스트의 threshold 예시

ClusterLoader2에는 일부 테스트에서 threshold를 지원합니다:

```yaml
# testing/load/modules/network-policy/net-policy-metrics.yaml

- name: PolicyCreation - Perc99
  query: histogram_quantile(0.99, ...)
  threshold: 5  # 5초 이상이면 실패로 표시

- name: Failed endpoint regenerations percentage
  query: sum(...fail...) / sum(...) * 100
  threshold: 0.01  # 0.01% 이상이면 실패
```

---

## 7. Fortio vs 현재 도구들: 무엇을 선택해야 하나?

### 7.1 도구별 특성 비교

```
┌─────────────────────────────────────────────────────────────────┐
│                      도구 비교표                                 │
├────────────┬────────────────┬────────────────┬─────────────────┤
│   특성     │    iperf       │    Siege       │    Fortio       │
├────────────┼────────────────┼────────────────┼─────────────────┤
│ 계층       │ L4 (TCP/UDP)   │ L7 (HTTP)      │ L7 (HTTP/gRPC)  │
├────────────┼────────────────┼────────────────┼─────────────────┤
│ 주요 측정  │ 대역폭, 지터   │ HTTP 응답시간   │ QPS, Latency   │
│            │ 손실률, 지연   │                │ Percentile      │
├────────────┼────────────────┼────────────────┼─────────────────┤
│ 부하 제어  │ 대역폭 고정    │ 동시 사용자 수  │ QPS 고정 ★     │
├────────────┼────────────────┼────────────────┼─────────────────┤
│ 결과 형식  │ 텍스트         │ 텍스트         │ JSON, 히스토그램│
├────────────┼────────────────┼────────────────┼─────────────────┤
│ 사용 사례  │ 네트워크 인프라│ 웹 서버 테스트  │ 마이크로서비스 │
│            │ CNI 성능 검증  │                │ Istio 성능 검증 │
├────────────┼────────────────┼────────────────┼─────────────────┤
│ Istio 연동 │ 없음           │ 없음           │ 공식 도구 ★    │
└────────────┴────────────────┴────────────────┴─────────────────┘
```

### 7.2 Fortio 상세

**Fortio**는 [Istio 프로젝트](https://github.com/fortio/fortio)에서 시작된 부하 테스트 도구입니다.

```
┌─────────────────────────────────────────────────────────────────┐
│                      Fortio 특징                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. QPS 기반 부하 제어                                          │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ fortio load -qps 100 -t 30s http://target:8080      │    │
│     │                                                      │    │
│     │ "초당 100개 요청을 정확히 30초 동안 보내라"           │    │
│     └─────────────────────────────────────────────────────┘    │
│                                                                 │
│  2. 정확한 Percentile 측정                                      │
│     ┌─────────────────────────────────────────────────────┐    │
│     │ P50: 2.3ms                                           │    │
│     │ P90: 5.1ms                                           │    │
│     │ P99: 12.4ms                                          │    │
│     │ P99.9: 45.2ms                                        │    │
│     └─────────────────────────────────────────────────────┘    │
│                                                                 │
│  3. HTTP/gRPC 프로토콜 지원                                     │
│                                                                 │
│  4. 내장 웹 UI로 실시간 결과 확인                               │
│                                                                 │
│  5. Prometheus 메트릭 내보내기                                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.3 언제 어떤 도구를 선택해야 하나?

```
┌─────────────────────────────────────────────────────────────────┐
│                    도구 선택 가이드                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  테스트 목적                          권장 도구                  │
│  ─────────────────────────────────────────────────────────      │
│                                                                 │
│  "CNI 플러그인 성능이 궁금하다"       → iperf (TCP/UDP)         │
│   (Calico vs Cilium vs Flannel)                                 │
│                                                                 │
│  "노드 간 네트워크 대역폭 확인"       → iperf (TCP)             │
│                                                                 │
│  "VoIP/게임 서버 품질 테스트"         → iperf (UDP)             │
│   (지터, 패킷 손실 중요)                                        │
│                                                                 │
│  "REST API 응답시간 테스트"           → Fortio 또는 Siege       │
│                                                                 │
│  "Istio/Service Mesh 성능"            → Fortio ★               │
│                                                                 │
│  "마이크로서비스 부하 테스트"         → Fortio ★               │
│   (QPS 제어, Percentile 중요)                                   │
│                                                                 │
│  "gRPC 서비스 테스트"                 → Fortio ★               │
│                                                                 │
│  "클러스터 전체 인수 테스트"          → ClusterLoader2          │
│   (네트워크 + Pod 스케줄링 +         (iperf + Siege 조합)      │
│    API 서버 등 종합)                                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 7.4 ClusterLoader2에 Fortio를 사용할 수 있나?

**가능합니다.** ClusterLoader2는 커스텀 측정 도구를 추가할 수 있습니다.

```yaml
# Fortio를 사용하는 커스텀 테스트 예시 (직접 구현 필요)

steps:
- name: Deploy Fortio pods
  phases:
  - objectTemplatePath: fortio-deployment.yaml

- name: Run Fortio load test
  measurements:
  - Method: Exec
    Params:
      command: "fortio load -qps 100 -t 30s http://target:8080"
```

**그러나**, 현재 ClusterLoader2의 네트워크 테스트는 iperf/Siege 기반으로 구현되어 있어 Fortio를 사용하려면 별도 개발이 필요합니다.

---

## 8. 인수 테스트로의 적합성

### 8.1 ClusterLoader2 네트워크 테스트의 장점

```
┌─────────────────────────────────────────────────────────────────┐
│              인수 테스트로서의 장점                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ✓ Kubernetes 네이티브                                          │
│    - CRD 기반 통신으로 K8s API 의존                             │
│    - Pod 스케줄링, 네트워크 정책 등 K8s 기능과 통합 테스트       │
│                                                                 │
│  ✓ 자동화된 Pod 페어링                                          │
│    - 다른 노드 간 통신 자동 구성                                 │
│    - 실제 서비스 환경 시뮬레이션                                 │
│                                                                 │
│  ✓ 동기화된 부하 생성                                           │
│    - 모든 Pod가 동시에 트래픽 생성                               │
│    - 일관된 테스트 조건                                          │
│                                                                 │
│  ✓ 다양한 프로토콜 지원                                         │
│    - TCP: 대역폭 측정                                           │
│    - UDP: 실시간 통신 품질                                      │
│    - HTTP: 애플리케이션 레벨 성능                               │
│                                                                 │
│  ✓ 스케일 테스트                                                │
│    - 1:1 (기준선) ~ N:M (대규모) 지원                           │
│    - Percentile 기반 통계                                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.2 인수 테스트 시 고려사항

```
┌─────────────────────────────────────────────────────────────────┐
│                    고려해야 할 사항                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. Pass/Fail 기준 정의 필요                                    │
│     - ClusterLoader2는 기본적으로 메트릭만 수집                 │
│     - 별도의 threshold 검증 로직 구현 필요                      │
│                                                                 │
│  2. 환경 변수 고려                                              │
│     - 노드 수, 네트워크 토폴로지에 따라 결과 변동               │
│     - 동일 환경에서 기준선(Baseline) 먼저 측정                   │
│                                                                 │
│  3. 테스트 조합 선택                                            │
│     - TCP 1:1: 기본 대역폭 확인                                 │
│     - UDP 1:1: 실시간 통신 품질                                 │
│     - TCP/UDP N:M: 대규모 동시 통신                             │
│                                                                 │
│  4. LoadBalancer 테스트                                         │
│     - 클라우드 환경에서만 유효                                  │
│     - 프로비저닝 시간은 클라우드 제공자 의존                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 8.3 권장 인수 테스트 체크리스트

```yaml
# 네트워크 인수 테스트 체크리스트

network_acceptance_tests:

  # 필수 테스트
  required:
    - name: "Pod 간 TCP 연결성"
      test: tcp-1:1
      pass_criteria: "연결 성공, Throughput > 0"

    - name: "Pod 간 기본 대역폭"
      test: tcp-1:1
      pass_criteria: "Throughput > 1000 kbytes/sec (약 8Mbps)"

    - name: "Service 도달성"
      test: ServiceCreationLatency
      pass_criteria: "모든 Service reachable"

  # 권장 테스트
  recommended:
    - name: "대규모 동시 통신"
      test: tcp-50:50
      pass_criteria: "Perc05/Perc50 > 0.7 (성능 일관성)"

    - name: "UDP 품질 (VoIP/게임용)"
      test: udp-1:1
      pass_criteria: "Jitter < 5ms, Loss < 0.1%"

    - name: "LoadBalancer 프로비저닝"
      test: l4lb
      pass_criteria: "생성 시간 < 2분"

  # 선택 테스트
  optional:
    - name: "NetworkPolicy 적용 시간"
      test: NetworkPolicyEnforcement
      pass_criteria: "Perc99 < 5초"
```

### 8.4 Fortio vs ClusterLoader2 결론

| 상황 | 권장 |
|------|------|
| **CNI/네트워크 인프라 인수 테스트** | ClusterLoader2 (iperf) |
| **마이크로서비스 API 성능 테스트** | Fortio |
| **Istio/Service Mesh 성능 테스트** | Fortio |
| **클러스터 전체 확장성 테스트** | ClusterLoader2 |
| **복합 (네트워크 + API + 스케줄링)** | ClusterLoader2 + Fortio 조합 |

---

## 부록: 참고 자료

### 도구 공식 문서

- [iperf 공식 사이트](https://iperf.fr/)
- [Siege 공식 사이트](https://www.joedog.org/siege-home/)
- [Fortio GitHub](https://github.com/fortio/fortio)
- [Fortio 공식 사이트](https://fortio.org/)

### ClusterLoader2 소스 코드

| 파일 | 설명 |
|------|------|
| `util-images/network/netperfbenchmark/pkg/worker/worker.go` | Worker Pod 메인 로직 |
| `util-images/network/netperfbenchmark/pkg/worker/util.go` | iperf/Siege 파싱 로직 |
| `util-images/network/netperfbenchmark/Dockerfile` | Worker 이미지 빌드 |

### 관련 프로젝트

- [Kubernetes perf-tests/network](https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf)
- [Fortio Operator for Kubernetes](https://github.com/verfio/fortio-operator)
- [k8s-iperf](https://github.com/InfuseAI/k8s-iperf)

---

*이 문서는 ClusterLoader2 및 netperfbenchmark 소스 코드 분석을 기반으로 작성되었습니다.*

**Sources:**
- [Fortio GitHub](https://github.com/fortio/fortio)
- [Fortio Official Site](https://fortio.org/)
- [Kubernetes perf-tests Network Benchmarks](https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf)
